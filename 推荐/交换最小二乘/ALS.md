# 1 什么是ALS

&emsp;&emsp;ALS 是交替最小二乘（`alternating least squares`）的简称。在机器学习中，`ALS`特指使用交替最小二乘求解的一个协同推荐算法。它通过观察到的所有用户给商品的打分，来推断每个用户的喜好并向用户推荐适合的商品。举个例子，我们看下面一个`8*8`的用户打分矩阵。

<div  align="center"><img src="imgs/ALS.1.1.png" width = "450" height = "300" alt="图片名称" align="center" /></div>

&emsp;&emsp;&emsp;这个矩阵的每一行代表一个用户（u1,u2,…,u8）、每一列代表一个商品（v1,v2,…,v8）、用户的打分为1-9分。这个矩阵只显示了观察到的打分，我们需要推测没有观察到的打分。比如（u6，v5）打分多少？如果以数独的方式来解决这个问题，可以得到唯一的结果。
因为数独的规则很强，每添加一条规则，就让整个系统的自由度下降一个量级。当我们满足所有的规则时，整个系统的自由度就降为`1`了，也就得出了唯一的结果。对于上面的打分矩阵，如果我们不添加任何条件的话，也即打分之间是相互独立的，我们就没法得到（u6，v5）的打分。
所以在这个用户打分矩阵的基础上，我们需要提出一个限制其自由度的合理假设，使得我们可以通过观察已有打分来猜测未知打分。

&emsp;&emsp;`ALS`的核心就是这样一个假设：打分矩阵是近似低秩的。换句话说，就是一个`m*n`的打分矩阵可以由分解的两个小矩阵`U（m*k）`和`V（k*n）`的乘积来近似，即\\(A\approx U{A}^{T}\\)。这就是ALS的矩阵分解方法。这样我们把系统的自由度从O(mn)降到了O((m+n)*k)。

&emsp;&emsp;那么ALS的低秩假设为什么是合理的呢？我们描述一个人的喜好经常是在一个抽象的低维空间上进行的，并不需要一一列出他喜好的事物。例如，我喜好看悬疑影片，可能代表我喜欢《神探夏洛特》、《神探狄仁杰》等。这些影片都符合我对自己喜好的描述，也就是说他们在这个抽象的低维空间的投影和我的喜好相似。再抽象一些来描述这个问题，我们把某个人的喜好映射到了低维向量ui 上，同时将某个影片的特征映射到了维度相同的向量vj上，那么这个人和这个影片的相似度就可以表述成这两个向量之间的内积。 我们把打分理解成相似度，那么打分矩阵A就可以由用户喜好矩阵和产品特征矩阵的乘积来近似了。

&emsp;&emsp;那么我们怎样选取这个低维空间呢？这个低维空间要能够很好的区分事物，那么就需要一个明确的可量化目标，这就是重构误差。在ALS中我们使用F范数来量化重构误差，就是每个元素重构误差的平方和。这里存在一个问题，我们只观察到部分打分，A中的大量未知元正是我们想推断的，所以这个重构误差是包含未知数的。解决方案很简单：只计算已知打分的重构误差，即。后面的章节我们将从原理上讲解spark中实现的ALS模型。

# 2 spark中ALS的实现原理


